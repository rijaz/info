{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Hotel Ratings on Booking # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will practice web scraping on the following [site](https://www.booking.com/searchresults.html?aid=304142&label=gen173nr-1DCAEoggJCAlhYSDNiBW5vcmVmcgV1c19tYYgBAZgBMcIBA2FibsgBDdgBA-gBAfgBApICAXmoAgQ&sid=28d97f630803f9d48b4a1f535cbdd33f&class_interval=1&dest_id=20061717&dest_type=city&group_adults=2&group_children=0&label_click=undef&no_rooms=1&raw_dest_type=city&room1=A%2CA&sb_price_type=total&src=index&src_elem=sb&ss=Boston&ssb=empty&ssne_untouched=Canc√∫n&rows=15). Let's get some basic information for each hotel in Boston.\n",
    "On each hotel page, scrape the following information: \n",
    "1. Hotel Name\n",
    "2. Class of Rating (Wonderful/Excellent/Very Good/Good)\n",
    "3. Rating Score\n",
    "4. Number of Reviews\n",
    "\n",
    "\n",
    "** Save the data in \"traveler_ratings.csv\" in the following format: hotel_name, class_of_rating, rating, num_reviews **\n",
    "\n",
    "**(10 pts)**\n",
    "\n",
    "You can see an overview of the information as displayed:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Information to be scraped](booking_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at csv file\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import codecs\n",
    "import json\n",
    "import re\n",
    "\n",
    "base_url = \"http://www.booking.com\"\n",
    "#Describes your browser\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\"\n",
    "\n",
    "datadir = \"data\"\n",
    "city = \"Boston\"\n",
    "hotel_links = []\n",
    "outfile = open('traveler_ratings.csv', 'w')\n",
    "outfile.write('hotel_name, class_of_rating, rating, num_reviews\\n')\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.ERROR)\n",
    "loghandler = logging.StreamHandler(sys.stderr)\n",
    "loghandler.setFormatter(logging.Formatter(\"[%(asctime)s] %(message)s\"))\n",
    "log.addHandler(loghandler)\n",
    "\n",
    "\n",
    "def get_city_page(city):\n",
    "    url = base_url + \"/\" + city\n",
    "    log.info(\"URL TO REQUEST: %s \\n\" % url)\n",
    "    headers = {'User-Agent' : user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "    \n",
    "    with open(os.path.join(city + '-search-page.json'), \"w\") as h:\n",
    "        h.write(html.decode(\"utf-8\").encode('cp850', 'replace').decode('cp850'))\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "    li = soup.findAll('a', href=True)\n",
    "    for el in li:\n",
    "        if el.find(text=re.compile('All')) and el.find(text=re.compile('properties in')):\n",
    "            return(el['href'])\n",
    "\n",
    "    log.info(\"City not listed\")\n",
    "\n",
    "    \n",
    "    \n",
    "def get_hotellist_page(city_url, count):\n",
    "    url = base_url + city_url\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "    \n",
    "    with open(os.path.join(datadir, city + '-hotellist-' + str(count) + '.html'), \"w\") as h:\n",
    "        h.write(html.decode(\"utf-8\").encode('cp850', 'replace').decode('cp850'))\n",
    "\n",
    "    return(html)\n",
    "\n",
    "\n",
    "\n",
    "def parse_hotellist_page(html, hotel_links):\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    " \n",
    "    hotel_boxes = soup.findAll('div', {'class' :re.compile('sr_item_default')})\n",
    "    \n",
    "    for hotel_box in hotel_boxes:\n",
    "        \n",
    "        state = True\n",
    "        try:\n",
    "            hotel_name = hotel_box.find('span', {'class' : 'sr-hotel__name'}).find(text=True).strip()\n",
    "            link = hotel_box.find('a', {'class' :'hotel_name_link url'})['href'].strip()\n",
    "            \n",
    "        except Exception:\n",
    "            state = False\n",
    "            log.error(\"Hotel not found\")\n",
    "            hotel_name = \"\"\n",
    "            class_of_rating = \"N/A\"\n",
    "            rating = \"N/A\"\n",
    "            num_reviews = \"N/A\"\n",
    "            link = \"\"\n",
    "        \n",
    "        if (state):\n",
    "            try:\n",
    "                class_of_rating = hotel_box.find('span', {'class' : 'review-score-widget__text'}).find(text=True).strip()\n",
    "                rating = hotel_box.find('span', {'class' :'review-score-badge'}).find(text=True).strip()  \n",
    "                num_reviews = hotel_box.find('span', {'class' : 'review-score-widget__subtext'}).find(text=True).strip()\n",
    "            \n",
    "            except Exception:\n",
    "                class_of_rating = \"N/A\"\n",
    "                rating = \"N/A\"         \n",
    "                num_reviews = \"N/A\"\n",
    "                \n",
    "        log.info(\"HOTEL NAME: %s\" % hotel_name)\n",
    "        log.info(\"HOTEL REVIEWS: %s\" % num_reviews)\n",
    "        log.info(\"HOTEL STAR RATING: %s \" % rating)\n",
    "        log.info(\"HOTEL CLASS OF RATINGS: %s \\n\" % class_of_rating)\n",
    "        \n",
    "        hotel_links += [link]\n",
    "        line = hotel_name.replace(',', '') + ',' + class_of_rating + ',' + rating + ',' + num_reviews.replace(',', '') + '\\n'\n",
    "        outfile.write(line)\n",
    "        \n",
    "    div = soup.find(\"div\", {\"class\" : \"results-paging\"})\n",
    "    if div.find('span', {'class':'paging-end'}):\n",
    "        log.info('We reached last page')\n",
    "        return False\n",
    "    hrefs = div.findAll('a', href=True)\n",
    "    \n",
    "    for href in hrefs:\n",
    "        if href.find(text=True) == 'Next page':\n",
    "            log.info(\"Next url is %s\" % href['href'])\n",
    "            return href['href']\n",
    "        \n",
    "city_url = get_city_page(city)\n",
    "\n",
    "c = 0\n",
    "page_urls = []\n",
    "while(city_url):\n",
    "    c += 1\n",
    "    h_page = get_hotellist_page(city_url, c)\n",
    "    city_url = parse_hotellist_page(h_page, hotel_links)\n",
    "    page_urls += [city_url]\n",
    "outfile.close()\n",
    "print(\"Look at csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's scrape some reviews. For each review of each each hotel in Boston you are to scrape the following attributes: \n",
    "1. Reviewer name\n",
    "2. Reviewer ethnicity\n",
    "3. Number of reviews \n",
    "4. Number of helpful votes\n",
    "5. Date\n",
    "6. Rating\n",
    "7. Negative Review\n",
    "8. Positive Review\n",
    "\n",
    "Note that you will also need the hotel's name!! Also, some reviews may not have all attributes. \n",
    "\n",
    "** Save the data in \"review_ratings.csv\" in the following format: hotel_name, reviewer_name, ethnicity, num_reviews, num_help_votes, date, rating, neg_review, pos_review **\n",
    "\n",
    "**(25 pts)**\n",
    "\n",
    "You can see an overview of the information as displayed:\n",
    "![Information to be scraped](review_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "got links\n"
     ]
    }
   ],
   "source": [
    "review_links = []\n",
    "block_display = []\n",
    "exp = []\n",
    "reviews_outfile = open(\"review_ratings.csv\", \"w\", encoding=\"utf-8\")\n",
    "reviews_outfile.write(\"Hotel Name, Reviewer Name, Reviewer Ethnicity, Number of Reviews, Date, Rating, Positive ReviewsS, Negative Reviews\\n\")\n",
    "\n",
    "def get_review_links(hotel_links, review_links, block_display): \n",
    "    c = 0\n",
    "    for hotel_url in hotel_links:\n",
    "        c += 1\n",
    "        print(c)\n",
    "        url = base_url + hotel_url\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        headers = { 'User-Agent' : user_agent }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        html = response.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "                \n",
    "        \n",
    "        try:\n",
    "            review_link = soup.find('a', {'class' : 'show_all_reviews_btn'})['href']\n",
    "    \n",
    "        except Exception:\n",
    "            block_display += [url]\n",
    "            \n",
    "        if (review_link == \"#blockdisplay4\"):\n",
    "            block_display += [url]\n",
    "        else:\n",
    "            review_links += [review_link]\n",
    "            \n",
    "\n",
    "            \n",
    "get_review_links(hotel_links, review_links, block_display)\n",
    "print(\"got links\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def extract_link_reviews(review_links):\n",
    "    for rv_link in review_links:\n",
    "\n",
    "        url = base_url + rv_link\n",
    "        \n",
    "        time.sleep(3)\n",
    "        \n",
    "        headers = {'User-Agent' : user_agent }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        html = response.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "        \n",
    "        ht_name = soup.find('a', {'class' : 'standalone_header_hotel_link'}).text.strip()\n",
    "        \n",
    "        \n",
    "        review_box = soup.findAll('li', {'class':'review_item clearfix '})\n",
    "        \n",
    "        for review in review_box:\n",
    "            try:\n",
    "                reviewer_name = review.find('span', {'itemprop' : 'name'}).find(text=True).strip()\n",
    "                reviewer_ethnicity = review.find('span', {'itemprop' : 'nationality'}).text.strip()\n",
    "                reviewer_num_reviews = review.find('div', {'class':'review_item_user_review_count'}).text.strip()\n",
    "                review_date = review.find('p', {'class':'review_item_date'}).text.strip()\n",
    "                review_rating = review.find('span', {'class':'review-score-badge'}).text.strip()\n",
    "                \n",
    "            \n",
    "            except Exception:\n",
    "                reviewer_name = \"N/A\"\n",
    "                reviewer_ethnicity = \"N/A\"\n",
    "                reviewer_num_reviews = \"N/A\"\n",
    "                review_date = \"N/A\"\n",
    "                review_rating = \"N/A\"\n",
    "            try:\n",
    "                neg_review = review.find('p', {'class':'review_neg'}).text.strip()\n",
    "                pos_review = review.find('p', {'class' : 'review_pos'}).text.strip()\n",
    "                \n",
    "            except Exception:\n",
    "                neg_review = \"N/A\"\n",
    "                pos_review = \"N/A\"\n",
    "                \n",
    "            line = ht_name.replace(',', '')+','+reviewer_name+','+reviewer_ethnicity+','+reviewer_num_reviews+','+review_date+','+review_rating+','+pos_review.replace('\\n', '')+','+neg_review.replace('\\n', '') + '\\n'\n",
    "            reviews_outfile.write(line)\n",
    "            \n",
    "if (review_links != 0):\n",
    "    extract_link_reviews(review_links)\n",
    "else:\n",
    "    print(\"no links\")\n",
    "\n",
    "reviews_outfile.close()\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews that were collected are the reviews of the hotels that have separate review pages as opposed to modal boxes.\n",
    "When trying to access the reviews from the modal boxes BeautifulSoup was not about the find the <li> tags that contained the information or even the <ul> tag that contained the <li> tags for the reviews.\n",
    "Thus in my code, I have separated the hotels by those that have the actual review page (review links) and those that don't and instead have the modal boxes (block_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
